{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: xgboost\n",
      "Warning message:\n",
      "\"package 'xgboost' was built under R version 3.4.3\""
     ]
    }
   ],
   "source": [
    "require(xgboost)\n",
    "# load in the agaricus dataset\n",
    "data(agaricus.train, package='xgboost')\n",
    "data(agaricus.test, package='xgboost')\n",
    "dtrain <- xgb.DMatrix(agaricus.train$data, label = agaricus.train$label)\n",
    "dtest <- xgb.DMatrix(agaricus.test$data, label = agaricus.test$label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: for customized objective function, we leave objective as default\n",
    "# note: what we are getting is margin value in prediction\n",
    "# you must know what you are doing\n",
    "watchlist <- list(eval = dtest, train = dtrain)\n",
    "num_round <- 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user define objective function, given prediction, return gradient and second order gradient\n",
    "# this is loglikelihood loss\n",
    "logregobj <- function(preds, dtrain) {\n",
    "  labels <- getinfo(dtrain, \"label\")\n",
    "  preds <- 1/(1 + exp(-preds))\n",
    "  grad <- preds - labels\n",
    "  hess <- preds * (1 - preds)\n",
    "  return(list(grad = grad, hess = hess))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user defined evaluation function, return a pair metric_name, result\n",
    "# NOTE: when you do customized loss function, the default prediction value is margin\n",
    "# this may make buildin evalution metric not function properly\n",
    "# for example, we are doing logistic loss, the prediction is score before logistic transformation\n",
    "# the buildin evaluation error assumes input is after logistic transformation\n",
    "# Take this in mind when you use the customization, and maybe you need write customized evaluation function\n",
    "evalerror <- function(preds, dtrain) {\n",
    "  labels <- getinfo(dtrain, \"label\")\n",
    "  err <- as.numeric(sum(labels != (preds > 0)))/length(labels)\n",
    "  return(list(metric = \"error\", value = err))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"start training with user customized objective\"\n",
      "[1]\teval-error:0.042831\ttrain-error:0.046522 \n",
      "[2]\teval-error:0.021726\ttrain-error:0.022263 \n"
     ]
    }
   ],
   "source": [
    "param <- list(max_depth=2, eta=1, nthread = 2, silent=1, \n",
    "              objective=logregobj, eval_metric=evalerror)\n",
    "print ('start training with user customized objective')\n",
    "# training with customized objective, we can also do step by step training\n",
    "# simply look at xgboost.py's implementation of train\n",
    "bst <- xgb.train(param, dtrain, num_round, watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"start training with user customized objective, with additional attributes in DMatrix\"\n",
      "[1]\teval-error:0.042831\ttrain-error:0.046522 \n",
      "[2]\teval-error:0.021726\ttrain-error:0.022263 \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# there can be cases where you want additional information \n",
    "# being considered besides the property of DMatrix you can get by getinfo\n",
    "# you can set additional information as attributes if DMatrix\n",
    "\n",
    "# set label attribute of dtrain to be label, we use label as an example, it can be anything \n",
    "attr(dtrain, 'label') <- getinfo(dtrain, 'label')\n",
    "# this is new customized objective, where you can access things you set\n",
    "# same thing applies to customized evaluation function\n",
    "logregobjattr <- function(preds, dtrain) {\n",
    "  # now you can access the attribute in customized function\n",
    "  labels <- attr(dtrain, 'label')\n",
    "  preds <- 1/(1 + exp(-preds))\n",
    "  grad <- preds - labels\n",
    "  hess <- preds * (1 - preds)\n",
    "  return(list(grad = grad, hess = hess))\n",
    "}\n",
    "param <- list(max_depth=2, eta=1, nthread = 2, silent=1, \n",
    "              objective=logregobjattr, eval_metric=evalerror)\n",
    "print('start training with user customized objective, with additional attributes in DMatrix')\n",
    "# training with customized objective, we can also do step by step training\n",
    "# simply look at xgboost.py's implementation of train\n",
    "bst <- xgb.train(param, dtrain, num_round, watchlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting from prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "require(xgboost)\n",
    "# load in the agaricus dataset\n",
    "data(agaricus.train, package='xgboost')\n",
    "data(agaricus.test, package='xgboost')\n",
    "dtrain <- xgb.DMatrix(agaricus.train$data, label = agaricus.train$label)\n",
    "dtest <- xgb.DMatrix(agaricus.test$data, label = agaricus.test$label)\n",
    "\n",
    "watchlist <- list(eval = dtest, train = dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"start running example to start from a initial prediction\"\n",
      "[1]\teval-error:0.042831\ttrain-error:0.046522 \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "TRUE"
      ],
      "text/latex": [
       "TRUE"
      ],
      "text/markdown": [
       "TRUE"
      ],
      "text/plain": [
       "[1] TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "TRUE"
      ],
      "text/latex": [
       "TRUE"
      ],
      "text/markdown": [
       "TRUE"
      ],
      "text/plain": [
       "[1] TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"this is result of boost from initial prediction\"\n",
      "[1]\teval-error:0.021726\ttrain-error:0.022263 \n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# advanced: start from a initial base prediction\n",
    "#\n",
    "print('start running example to start from a initial prediction')\n",
    "# train xgboost for 1 round\n",
    "param <- list(max_depth=2, eta=1, nthread = 2, silent=1, objective='binary:logistic')\n",
    "bst <- xgb.train(param, dtrain, 1, watchlist)\n",
    "# Note: we need the margin value instead of transformed prediction in set_base_margin\n",
    "# do predict with output_margin=TRUE, will always give you margin values before logistic transformation\n",
    "ptrain <- predict(bst, dtrain, outputmargin=TRUE)\n",
    "ptest  <- predict(bst, dtest, outputmargin=TRUE)\n",
    "# set the base_margin property of dtrain and dtest\n",
    "# base margin is the base prediction we will boost from\n",
    "setinfo(dtrain, \"base_margin\", ptrain)\n",
    "setinfo(dtest, \"base_margin\", ptest)\n",
    "\n",
    "print('this is result of boost from initial prediction')\n",
    "bst <- xgb.train(params = param, data = dtrain, nrounds = 1, watchlist = watchlist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R [Anaconda3]",
   "language": "R",
   "name": "R [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
